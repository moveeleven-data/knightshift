version: "3.9"   # dockerâ€‘compose schema version
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  KnightShift stack:  Postgres 17  +  custom pipeline image
#                      +  ApacheÂ Airflow for orchestration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

x-common-env: &common_env   # ğŸ”— anchor reused by multiple services
  PGHOST:     ${PGHOST}
  PGPORT:     ${PGPORT}
  PGDATABASE: ${PGDATABASE}
  PGUSER:     ${PGUSER}
  PGPASSWORD: ${PGPASSWORD}

  AWS_ACCESS_KEY_ID:     ${AWS_ACCESS_KEY_ID}
  AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
  AWS_DEFAULT_REGION:    ${AWS_DEFAULT_REGION}
  DB_SECRET_NAME:        ${DB_SECRET_NAME}

services:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  db:
    image: postgres:17
    restart: always
    environment:
      POSTGRES_USER:     ${PGUSER}
      POSTGRES_PASSWORD: ${PGPASSWORD}
      POSTGRES_DB:       ${PGDATABASE}
    ports:
      # expose containerâ€‘5432 on localhost:55432 (avoids clashing with
      # a host Postgres installation)
      - "127.0.0.1:55432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./schemas:/docker-entrypoint-initdb.d   # bootâ€‘timeÂ SQL seeds

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  pipeline:
    build: .
    depends_on: [db]
    environment: *common_env      # â† expands the anchor defined above
    volumes:
      - ./logs:/app/logs          # persistent pipeline logs
    command: ["bash", "run.sh"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  airflow:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    restart: always
    depends_on: [db]
    environment:
      <<: *common_env             # inherit DB / AWS creds
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@db:5432/knightshift
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      RUNNING_IN_DOCKER: "true"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/app/src              # source code for live reload
      - ./config:/app/config        # env + config bundle
    ports:
      - "8080:8080"
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin
                            --firstname admin --lastname admin
                            --role Admin --email admin@example.com &&
        airflow webserver & airflow scheduler
      "

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
volumes:
  pg_data:   # named volume â†’ survives container rebuilds
