# â™ KnightShift: Lichess Data Pipeline

KnightShift is a production-style chess data pipeline that ingests real-time
Lichess TV games, parses PGN data, and stores structured game records in a
PostgreSQL database.

This project simulates real-world data engineering practicesâ€”secure
secrets management, stream-style ingestion, schema-aware transformation,
and upsert logic.

---

## ğŸ§  What It Does

- Streams live chess games from multiple Lichess TV channels (e.g. blitz, rapid, horde).
- Parses PGN-formatted chess data into structured records.
- Transforms raw PGN into a clean schema: player names, ratings, results, time control, etc.
- Performs upserts into PostgreSQL using SQLAlchemy Core.
- Uses AWS Secrets Manager to securely load DB credentials.

---

## ğŸ›  Tech Stack

- **Python 3.10+**
- **PostgreSQL**
- **SQLAlchemy Core**
- **AWS Secrets Manager** (`boto3`)
- **Requests** (API interaction)

---

## ğŸ“ Project Structure

```
knightshift/
â”œâ”€â”€ logs/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ CHANGELOG.md
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ schema_reference.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ db/   
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ game_upsert.py
â”‚   â”œâ”€â”€ pipeline/    
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ run_cleaning.py
â”‚   â”‚   â”œâ”€â”€ run_enrichment.py
â”‚   â”‚   â””â”€â”€ run_ingestion.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ get_games_from_tv.py
â”‚   â”œâ”€â”€ cleaning/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ validate_tv_channel_games.py
â”‚   â”œâ”€â”€ enrichment/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ backfill_user_profiles.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ logging_utils.py
â”‚   â”‚   â”œâ”€â”€ pgn_parser.py
â”‚   â”‚   â”œâ”€â”€ db_utils.py
â”‚   â”‚   â””â”€â”€ init__.py
â”‚   â”œâ”€â”€ legacy/
â”‚   â”‚   â”œâ”€â”€ check_urls_of_games.py
â”‚   â”‚   â”œâ”€â”€ update_all_games.py
â”‚   â”‚   â””â”€â”€ get_games_from_users.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ .env.local
â”‚   â””â”€â”€ .env.docker
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ test_pgn_parser.py
â”‚   â”œâ”€â”€ test_db_utils.py
â”‚   â”œâ”€â”€ test_get_games_from_tv.py
â”‚   â””â”€â”€ test_validation_logic.py
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ CHANGELOG.md
â”‚   â”œâ”€â”€ lichess_users.sql
â”‚   â””â”€â”€ tv_channel_games.sql
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ run_knightshift.bat
â””â”€â”€ run.sh
```

---

## ğŸ” Secrets Management

Credentials are securely pulled from AWS Secrets Manager.

Expected secret format:

```json
{
  "PGHOST": "your-db-host",
  "PGPORT": "5432",
  "PGDATABASE": "your-db-name",
  "PGUSER": "your-username",
  "PGPASSWORD": "your-password"
}
```

---

## â–¶ï¸ How to Run

1. Create a Postgres database and store credentials in AWS Secrets Manager.

2. Install Python dependencies:

   ```
   pip install -r requirements.txt
   ```

3. Run the pipeline:

   ```
   python main.py
   ```

The pipeline fetches Lichess games every 40 seconds for 5 hours, 
processes them, and stores them in your Postgres database.

---

## ğŸš§ Future Expansion: Chess Analytics Pipeline (Multi-Source)

This project will evolve into a full **Chess Analytics Pipeline** with:

### âœˆï¸ Multiple Data Sources

- **Lichess (Real-Time):** PGN from TV stream and export API
- **Kaggle Archive:** Millions of historical games
- **FIDE Ratings:** Public CSVs with official player data

### âš™ï¸ Architecture Components

- **Staging in RDS (PostgreSQL)**
- **Raw Data in S3 Buckets**
- **Transformation Jobs via AWS Glue or Python**
- **Partitioned Tables & Concurrency Controls**
- **Analytics Layer in Redshift or a second Postgres**
- **Dashboards with AWS QuickSight**

### ğŸ— Planned Expansion by Month

**Month 1:**

- Refactor ingestion scripts
- Local Docker + Postgres setup
- Create first working Dockerfile

**Month 2:**

- Automate ingestion via cron or Airflow (locally)
- Add simple data validation (e.g. ELO range checks)

**Month 3:**

- Partition Postgres by date
- Add concurrency safety (transactions or row locks)
- Add full PGN enrichment via Lichess export API

**Month 4:**

- Deploy ingestion & enrichment in Kubernetes (Minikube/Kind)
- Run Airflow inside K8s cluster

**Month 5:**

- Add Great Expectations for data quality
- Monitor jobs with Prometheus & Grafana

**Month 6:**

- Load to Redshift or warehouse instance
- Create aggregated views (fact/dimension tables)
- Build public dashboards with QuickSight

---

## ğŸ“Œ Final Goal

Build a robust, multi-source chess data pipeline capable of:

- Continuous real-time ingestion
- Historical + official data merging
- Schema evolution
- Analytics-ready warehousing
- Production-grade monitoring
- BI dashboards

---

## ğŸ“… Built By

[Matthew Tripodi](https://github.com/okv627)

Let me know if youâ€™d like to see the public dashboard or learn more about
the architecture behind KnightShift.